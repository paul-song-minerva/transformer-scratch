{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import math, matmul, reshape, shape, transpose, cast, float32\n",
    "from tensorflow.keras.layers import Dense, Layer\n",
    "from keras.backend import softmax\n",
    " \n",
    "# Implementing the Scaled-Dot Product Attention\n",
    "class DotProductAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(DotProductAttention, self).__init__(**kwargs)\n",
    " \n",
    "    def call(self, queries, keys, values, d_k, mask=None):\n",
    "        # Scoring the queries against the keys after transposing the latter, and scaling\n",
    "        scores = matmul(queries, keys, transpose_b=True) / math.sqrt(cast(d_k, float32))\n",
    " \n",
    "        # Apply mask to the attention scores\n",
    "        if mask is not None:\n",
    "            scores += -1e9 * mask\n",
    " \n",
    "        # Computing the weights by a softmax operation\n",
    "        weights = softmax(scores)\n",
    " \n",
    "        # Computing the attention by a weighted sum of the value vectors\n",
    "        return matmul(weights, values)\n",
    "\n",
    "# Implementing the Multi-Head Attention\n",
    "class MultiHeadAttention(Layer):\n",
    "    def __init__(self, h, d_k, d_v, d_model, **kwargs):\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "        self.attention = DotProductAttention()  # Scaled dot product attention\n",
    "        self.heads = h  # Number of attention heads to use\n",
    "        self.d_k = d_k  # Dimensionality of the linearly projected queries and keys\n",
    "        self.d_v = d_v  # Dimensionality of the linearly projected values\n",
    "        self.d_model = d_model  # Dimensionality of the model\n",
    "        self.W_q = Dense(d_k)  # Learned projection matrix for the queries\n",
    "        self.W_k = Dense(d_k)  # Learned projection matrix for the keys\n",
    "        self.W_v = Dense(d_v)  # Learned projection matrix for the values\n",
    "        self.W_o = Dense(d_model)  # Learned projection matrix for the multi-head output\n",
    " \n",
    "    def reshape_tensor(self, x, heads, flag):\n",
    "        if flag:\n",
    "            # Tensor shape after reshaping and transposing: (batch_size, heads, seq_length, -1)\n",
    "            x = reshape(x, shape=(shape(x)[0], shape(x)[1], heads, -1))\n",
    "            x = transpose(x, perm=(0, 2, 1, 3))\n",
    "        else:\n",
    "            # Reverting the reshaping and transposing operations: (batch_size, seq_length, d_k)\n",
    "            x = transpose(x, perm=(0, 2, 1, 3))\n",
    "            x = reshape(x, shape=(shape(x)[0], shape(x)[1], self.d_k))\n",
    "        return x\n",
    " \n",
    "    def call(self, queries, keys, values, mask=None):\n",
    "        # Rearrange the queries to be able to compute all heads in parallel\n",
    "        q_reshaped = self.reshape_tensor(self.W_q(queries), self.heads, True)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    " \n",
    "        # Rearrange the keys to be able to compute all heads in parallel\n",
    "        k_reshaped = self.reshape_tensor(self.W_k(keys), self.heads, True)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    " \n",
    "        # Rearrange the values to be able to compute all heads in parallel\n",
    "        v_reshaped = self.reshape_tensor(self.W_v(values), self.heads, True)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    " \n",
    "        # Compute the multi-head attention output using the reshaped queries, keys and values\n",
    "        o_reshaped = self.attention(q_reshaped, k_reshaped, v_reshaped, self.d_k, mask)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    " \n",
    "        # Rearrange back the output into concatenated form\n",
    "        output = self.reshape_tensor(o_reshaped, self.heads, False)\n",
    "        # Resulting tensor shape: (batch_size, input_seq_length, d_v)\n",
    " \n",
    "        # Apply one final linear projection to the output to generate the multi-head attention\n",
    "        # Resulting tensor shape: (batch_size, input_seq_length, d_model)\n",
    "        return self.W_o(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class PositionEmbeddingFixedWeights(Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, output_dim, **kwargs):\n",
    "        super(PositionEmbeddingFixedWeights, self).__init__(**kwargs)\n",
    "        word_embedding_matrix = self.get_position_encoding(vocab_size, output_dim)   \n",
    "        position_embedding_matrix = self.get_position_encoding(sequence_length, output_dim)                                          \n",
    "        self.word_embedding_layer = Embedding(\n",
    "            input_dim=vocab_size, output_dim=output_dim,\n",
    "            weights=[word_embedding_matrix],\n",
    "            trainable=False\n",
    "        )\n",
    "        self.position_embedding_layer = Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim,\n",
    "            weights=[position_embedding_matrix],\n",
    "            trainable=False\n",
    "        )\n",
    "             \n",
    "    def get_position_encoding(self, seq_len, d, n=10000):\n",
    "        P = np.zeros((seq_len, d))\n",
    "        for k in range(seq_len):\n",
    "            for i in np.arange(int(d/2)):\n",
    "                denominator = np.power(n, 2*i/d)\n",
    "                P[k, 2*i] = np.sin(k/denominator)\n",
    "                P[k, 2*i+1] = np.cos(k/denominator)\n",
    "        return P\n",
    " \n",
    " \n",
    "    def call(self, inputs):        \n",
    "        position_indices = tf.range(tf.shape(inputs)[-1])\n",
    "        embedded_words = self.word_embedding_layer(inputs)\n",
    "        embedded_indices = self.position_embedding_layer(position_indices)\n",
    "        return embedded_words + embedded_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LayerNormalization, Layer, Dense, ReLU, Dropout\n",
    "\n",
    "# Implementing the Add & Norm Layer\n",
    "class AddNormalization(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AddNormalization, self).__init__(**kwargs)\n",
    "        self.layer_norm = LayerNormalization()  # Layer normalization layer\n",
    "\n",
    "    def call(self, x, sublayer_x):\n",
    "        # The sublayer input and output need to be of the same shape to be summed\n",
    "        add = x + sublayer_x\n",
    "\n",
    "        # Apply layer normalization to the sum\n",
    "        return self.layer_norm(add)\n",
    "\n",
    "# Implementing the Feed-Forward Layer\n",
    "class FeedForward(Layer):\n",
    "    def __init__(self, d_ff, d_model, **kwargs):\n",
    "        super(FeedForward, self).__init__(**kwargs)\n",
    "        self.fully_connected1 = Dense(d_ff)  # First fully connected layer\n",
    "        self.fully_connected2 = Dense(d_model)  # Second fully connected layer\n",
    "        self.activation = ReLU()  # ReLU activation layer\n",
    "\n",
    "    def call(self, x):\n",
    "        # The input is passed into the two fully-connected layers, with a ReLU in between\n",
    "        x_fc1 = self.fully_connected1(x)\n",
    "\n",
    "        return self.fully_connected2(self.activation(x_fc1))\n",
    "\n",
    "# Implementing the Encoder Layer\n",
    "class EncoderLayer(Layer):\n",
    "    def __init__(self, h, d_k, d_v, d_model, d_ff, rate, **kwargs):\n",
    "        super(EncoderLayer, self).__init__(**kwargs)\n",
    "        self.multihead_attention = MultiHeadAttention(h, d_k, d_v, d_model)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.add_norm1 = AddNormalization()\n",
    "        self.feed_forward = FeedForward(d_ff, d_model)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "        self.add_norm2 = AddNormalization()\n",
    "\n",
    "    def call(self, x, padding_mask, training):\n",
    "        # Multi-head attention layer\n",
    "        multihead_output = self.multihead_attention(x, x, x, padding_mask)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "        # Add in a dropout layer\n",
    "        multihead_output = self.dropout1(multihead_output, training=training)\n",
    "\n",
    "        # Followed by an Add & Norm layer\n",
    "        addnorm_output = self.add_norm1(x, multihead_output)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "        # Followed by a fully connected layer\n",
    "        feedforward_output = self.feed_forward(addnorm_output)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "        # Add in another dropout layer\n",
    "        feedforward_output = self.dropout2(feedforward_output, training=training)\n",
    "\n",
    "        # Followed by another Add & Norm layer\n",
    "        return self.add_norm2(addnorm_output, feedforward_output)\n",
    "\n",
    "# Implementing the Encoder\n",
    "class Encoder(Layer):\n",
    "    def __init__(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate, **kwargs):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "        self.pos_encoding = PositionEmbeddingFixedWeights(sequence_length, vocab_size, d_model)\n",
    "        self.dropout = Dropout(rate)\n",
    "        self.encoder_layer = [EncoderLayer(h, d_k, d_v, d_model, d_ff, rate) for _ in range(n)]\n",
    "\n",
    "    def call(self, input_sentence, padding_mask, training):\n",
    "        # Generate the positional encoding\n",
    "        pos_encoding_output = self.pos_encoding(input_sentence)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "        # Add in a dropout layer\n",
    "        x = self.dropout(pos_encoding_output, training=training)\n",
    "\n",
    "        # Pass on the positional encoded values to each encoder layer\n",
    "        for i, layer in enumerate(self.encoder_layer):\n",
    "            x = layer(x, padding_mask, training)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 1.3093830e+00  1.1566271e+00 -1.0458745e+00 ... -1.0213436e+00\n",
      "    8.8668770e-01 -1.3768914e-01]\n",
      "  [ 9.0674835e-01  8.0782330e-01 -1.1278911e+00 ... -7.0858651e-01\n",
      "   -1.2044011e-01 -5.3577387e-01]\n",
      "  [ 7.8658819e-01  7.2081113e-01 -1.2307107e+00 ... -5.7468790e-01\n",
      "    3.4069163e-01  4.8095495e-01]\n",
      "  [ 7.0240486e-01 -3.9930728e-01 -9.0943718e-01 ... -6.8387848e-01\n",
      "    9.8137486e-01 -4.8406532e-01]\n",
      "  [ 1.0598236e+00  1.4133768e-01 -1.2023852e+00 ... -1.3951480e+00\n",
      "    5.9606671e-01 -2.0088762e-01]]\n",
      "\n",
      " [[ 6.6831988e-01  8.9802897e-01 -1.0743164e+00 ... -8.1937540e-01\n",
      "    1.0309871e+00  1.8502121e-01]\n",
      "  [ 7.4175769e-01 -6.8128961e-01 -9.6750724e-01 ... -4.9936381e-01\n",
      "    3.7366048e-01  3.4630942e-01]\n",
      "  [ 1.0861148e+00 -3.0517945e-01 -8.9855772e-01 ... -4.7737312e-01\n",
      "    1.1093172e+00 -1.1147555e+00]\n",
      "  [ 3.9401716e-01  8.3398640e-01 -9.5536017e-01 ... -8.1977946e-01\n",
      "    9.7346926e-01 -1.0660794e+00]\n",
      "  [ 6.9281906e-01 -1.4134750e-01 -7.0664740e-01 ... -1.0030797e-01\n",
      "    4.5456326e-01 -1.0805528e+00]]\n",
      "\n",
      " [[ 4.8631519e-01  6.8171591e-01 -7.8972375e-01 ... -5.0965756e-01\n",
      "    1.2947795e+00 -3.3892092e-01]\n",
      "  [ 6.8028629e-01  3.2456353e-01 -7.7973396e-01 ...  1.6181043e-01\n",
      "    1.8106710e+00  4.4673356e-01]\n",
      "  [ 6.4228696e-01 -4.7542185e-01 -7.4728757e-01 ... -2.9199252e-01\n",
      "    1.8697361e+00 -7.8968415e-03]\n",
      "  [ 4.9426165e-01 -1.0734967e+00 -1.4059086e+00 ... -1.4383601e-01\n",
      "    1.6648852e+00  1.9086948e-01]\n",
      "  [ 7.6720965e-01 -4.5933641e-02 -6.8441993e-01 ... -8.2315373e-01\n",
      "   -5.8706373e-01 -2.3382838e-01]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 1.3584285e+00  6.2430203e-01 -7.0367444e-01 ... -6.4358538e-01\n",
      "    6.5620315e-01  1.3263658e-01]\n",
      "  [ 4.3709835e-01  1.2870138e+00 -8.7726688e-01 ... -4.7531852e-01\n",
      "    9.0173358e-01 -1.7843880e-01]\n",
      "  [ 1.0765016e+00  1.3980700e+00 -9.1006798e-01 ... -5.8433110e-01\n",
      "    7.5691581e-01  1.2688006e-01]\n",
      "  [ 2.0332594e+00  8.6066425e-01 -1.3175793e+00 ... -9.0429038e-01\n",
      "    1.0789236e+00  3.0487874e-01]\n",
      "  [ 1.5702193e+00  1.2292460e+00 -9.4247323e-01 ... -2.5318909e-01\n",
      "    1.0818440e+00 -4.5774457e-01]]\n",
      "\n",
      " [[ 2.1293803e-01  9.9438894e-01 -5.0046712e-01 ... -6.3392508e-01\n",
      "    1.0004812e+00 -7.1649814e-01]\n",
      "  [ 6.1080363e-02 -3.6867574e-02 -1.5202157e+00 ... -5.0896913e-01\n",
      "    9.9959910e-01 -1.0182258e+00]\n",
      "  [ 5.6835300e-01  1.2306227e-02 -5.2827728e-01 ... -5.3850014e-04\n",
      "    9.0474176e-01  4.2452989e-03]\n",
      "  [ 1.9324648e-01  8.2909435e-02 -1.4319013e+00 ... -4.0897360e-01\n",
      "    8.3880234e-01 -9.0246201e-02]\n",
      "  [ 6.7688890e-02  3.0023652e-01 -3.9868021e-01 ... -7.6810265e-01\n",
      "    3.8051721e-01 -7.2993463e-01]]\n",
      "\n",
      " [[ 1.3629229e+00  1.3172190e-01 -1.4595219e+00 ... -7.0179987e-01\n",
      "    8.1784137e-02  2.8733958e-02]\n",
      "  [ 1.3777521e+00  3.7911132e-01 -1.0014763e+00 ... -4.5582023e-01\n",
      "    5.5510086e-01 -3.2725018e-01]\n",
      "  [ 1.2291961e+00  8.6224645e-01 -1.6075624e+00 ... -8.3946472e-01\n",
      "    6.0005105e-01  3.0110652e-02]\n",
      "  [ 1.0336593e+00  5.7814818e-02 -1.6161678e+00 ... -2.1124005e-02\n",
      "    4.8730305e-01 -4.1325417e-01]\n",
      "  [ 8.5057920e-01 -6.6267914e-01 -1.9922353e+00 ... -8.4249139e-01\n",
      "    4.5927539e-01  3.6068425e-01]]], shape=(64, 5, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from numpy import random\n",
    "\n",
    "enc_vocab_size = 20 # Vocabulary size for the encoder\n",
    "input_seq_length = 5  # Maximum length of the input sequence\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "d_ff = 2048  # Dimensionality of the inner fully connected layer\n",
    "d_model = 512  # Dimensionality of the model sub-layers' outputs\n",
    "n = 6  # Number of layers in the encoder stack\n",
    "\n",
    "batch_size = 64  # Batch size from the training process\n",
    "dropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers\n",
    "\n",
    "input_seq = random.random((batch_size, input_seq_length))\n",
    "\n",
    "encoder = Encoder(enc_vocab_size, input_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n",
    "print(encoder(input_seq, None, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
